\documentclass[document.tex]{subfiles}

\begin{document}
\section{Системы линейных уравнений с переменными коэффициентами}
\begin{definition}
    Рассмотрим следующую систему уравнений
    \[
        y'(x) = A(x)y(x) + f(x)
    \],
    где $y$ и $f$ -- вектор функции, $A$ -- функция-матрица. $A = (a_{i, j}(x))$. Кроме того, считаем $A(x), f(x)$
    непрерывными на $[\alpha, \beta]$.

    Эта система называется системой линейных уравнений с переменными коэффициентами. Если $f(x) \equiv 0$, то система
    называется однородной.
\end{definition}

\begin{lemma}[Принцип суперпозиции для системы с переменными коэффициентами]
    Пусть $y_1, y_2$ -- два решения однородной системы $y'(x) = A(x)y(x)$. Тогда $\forall \lambda, \mu \in
    \mathbb{C}: \lambda y_1(x) + \mu y_2(x)$ является решением системы $y' = A(x)y(x)$
\end{lemma}

\begin{proof}
    $(\lambda y_1 + \mu y_2)' = \lambda y_1' + \mu y_2' = \lambda A(x) y_1 + \mu A(x) y_2 =  A(x) (\lambda y_1 + \mu
    y_2)$
\end{proof}

\begin{remark}
    Это показывает, что пространство решений является линейным пространством. Было бы неплохо отыскать там базис в
    каком-либо виде.
\end{remark}

\begin{definition}
    Вектор функции $y_1, \cdots, y_k$ называются линейно-зависимыми на отрезке $[\alpha, \beta]$, если существует такой
    набор $0 \neq (\lambda_1, \cdots, \lambda_k) \in \mathbb{C}$, что $\lambda_1 y_1 + \cdots + \lambda_k y_k \equiv 0$
    на $[\alpha, \beta]$. В противном случае назовем функции линейно-независимыми на $[\alpha, \beta]$
\end{definition}

\begin{corollary}
    Для линейно-зависимых на отрезке $[\alpha, \beta]$ вектор функций $y_1, \cdots y_k$ для любого $x_0 \in [\alpha,
    \beta]$ система векторов $y_1(x_0), \cdots, y_k(x_0)$ также будет линейно-зависимой. Обратное в общем случае неверно.
    Например, $y_1 = (x, x), y_2 = (x^2, x^2)$
\end{corollary}

\begin{definition}
    Пусть $y_1(x), \cdots, y_n(x)$ -- вектор-функции с $n$ компонентрами. Функция
    \[
    W(x) = W(y_1(x), \cdots, y_n(x)) = det
    \left(
        \begin{matrix}
            y_1^1(x) & \cdots & y_n^1(x) \\
            \cdots \\ & \cdots & \cdots \\
            y_1^n(x) & \cdots  & y_n^n(x)
        \end{matrix}
    \right)
    \] называется определителем Вронского или Вронцкианом
\end{definition}

\begin{lemma}
    Если Вронцкиан вектор функций $y_1, \cdots, y_n$ отличен от нуля хотя бы в одной точке отрезка $[\alpha, \beta]$, то
    $y_1, \cdots, y_n$ -- линейно-независимы.
\end{lemma}
\begin{proof}
    Пусть $x_0$ -- точка, такая, что $W(x_0) \neq 0$. Пусть $y_1, \cdots, y_n$ -- линейно-независимая система. Тогда
    $\exists \alpha_1, \cdots, \alpha_n : \alpha_1 y_1 + \cdots + \alpha_n y_n \equiv 0$ на $[\alpha, \beta]$. Это
    означает, что для точки $x_0$ выполнено: $\alpha_1 y_1(x_0) + \cdots + \alpha_n y_n(x_0) = 0$. Но тогда матрица, от
    которой считается определитель Вронского, является вырожденной. Тогда $W(x_0) = 0$. Противоречие.
\end{proof}

\begin{lemma}
    Если $y_1, \cdots, y_n$ линейно зависимые, то $W(x) \equiv 0$ на $[\alpha, \beta]$
\end{lemma}

\begin{theorem}
    Пусть $y_1, \cdots, y_n$ -- решение однородной системы линейных уравнений. Если существует $x_0 \in [\alpha,
    \beta]$, такая что $W(x_0) \neq 0$, то $y_1, \cdots, y_n$ линейно-независимы на отрезке $[\alpha, \beta]$
\end{theorem}

\begin{theorem}[Формула Лиувилля-Остроградского]
    Пусть $W(x)$ -- вронцкиан решений системы $y' = A(x)y$. Пусть $x_0 \in [\alpha, \beta]$. Тогда $\forall x \in
    [\alpha, \beta]$ имеет место формула:
    \[
        W(x) = W(x_0) \cdot \exp \left[ \int_{x_0}^{x} tr A(\tau) d\tau \right]
    \],
    где $tr A = \sum_{i = 0}^n a_{i, i}$ -- след матрицы
\end{theorem}

\begin{proof}
    \[
        \left|
            \begin{matrix}
                y_{11} & \cdots & y_{1n} \\
                \cdots & \cdots & \cdots \\
                y_{n1} & \cdots & y_{nn} \\
            \end{matrix}
        \right| = W(x)
    \]

    Пусть $W_{ij}(x)$ -- алгебраическое дополнение к элементу $y_{ij}$.

    Напоминание: $W_{ij}(x) = (-1)^{i + j} M_{ij}$, где
    $M_{ij}(x)$ -- минор элемента $y_{ij}$, то есть определитель матрицы $W$, из которой вычеркнули $i$-тую строчку и
    $j$-тую строку.

    Тогда $W(x) = \sum_{j = 1}^n y_{ij}W_{ij}(x)$. Заметим, что частная производная Вронцкиана по $y_{ij}$ -- это
    $W_{ij}$.

    Теперь найдем $y'_{ij}$. Мы знаем, что $y_j' = A(x)y_j$. Тогда $y'_{ij} = \sum_{k = 1}^n a_{ik} \cdot
    y_{kj}$
    \begin{multline*}
        W'(x) = \sum_{i, j = 1}^n \frac{\partial W}{\partial y_{ij}} \frac{d y_{ij}}{dx} = \sum_{i, j = 1}^n
        W_{ij} \sum_{k = 1}^n a_{ik}y_{kj} = \sum_{i, k = 1}^n a_{ik} \sum_{j = 1}^n y_{kj} W_{ij} = \\
        \sum_{i = 1}^n a_{ii} \cdot \sum_{j = 1}^n y_{ij} W_{ij}(x) = tr A(x) \cdot W(x)
    \end{multline*}
    Получается, что $W(x)$ удовлетворяет дифференциальному уравнению $W'(x) = tr A(x) \cdot W(x)$. Решаем его и
    получаем утверждение теоремы.

\end{proof}

\subsection{Фундаментальные системы решений линейной однородной системы дифференицальных уравнений с переменными
    коэффициентами}

\begin{definition}
    Фундаментальной системой решений $y' = A(x)y$ называется набор из $n$ линейно-независимых решений этой системы.
\end{definition}

\begin{theorem}
    Для любой системы однородных дифференциальных уравнений $y' = A(x)y$ существует фундаментальная система решений.
\end{theorem}

\begin{proof}
    Рассмотрим набор из $n$ линейно-независимых постоянных векторов $\hat y_1, \cdots, \hat y_n$. Зафиксируем $x_0 \in
    [\alpha, \beta]$. Рассмотрим серию задач Коши для $k \in \{1, \cdots, n\}$
    \[
        \begin{cases}
            y' = A(x)y, \\
            y(x_0) = \hat y_k
        \end{cases}
    \]

    В качестве решений получаем набор различных вектор функций $\{\varphi_i(x)\}_{i = 1}^n$, каждая из которых является
    решением задачи Коши. То есть $\forall i \in \{1, \cdots, n\}: \varphi_i' = A(x) \varphi_i, \varphi_i(x_0) = \hat
    y_k$. Допустим, что система этих вектор-функций оказалась линейно зависимой. То есть $\exists (\alpha_1, \cdots,
    \alpha_n) \neq 0: \alpha_1 \varphi_1 + \cdots + \alpha_n \varphi_n \equiv 0$ на $[\alpha, \beta]$. Следовательно,
    при $x = x_0$: $\exists (\alpha_1, \cdots, \alpha_n) \neq 0: \alpha_1 \hat y_1 + \ldots + \alpha_n \hat y_n = 0$. А
    это противоречит тому, что $(y_1, \cdots, y_n)$ -- ЛНЗ.
\end{proof}
\begin{remark}
    Таких фундамнетальных систем как минимум столько же, сколько и ЛНЗ систем из векторов.
\end{remark}

\begin{theorem}
    Пусть $y_1, \cdots, y_n$ -- ФСР для $y' = A(x)y$. Тогда любое решение $y(x)$ системы $y' = A(x)y$ единственным
    образом представляется в виде линейной комбинации векторов ФСР.
\end{theorem}

\begin{proof}
    Рассмотрим точку $x_0$. Обозначим $y_1(x_0) = \hat y_1, \cdots, y_n(x_0) = \hat y_n$. Этот набор векторов является
    линейно-независимыми. Следовательно
    \[
        \exists ! (a_1, \cdots, a_n): y(x_0) = \sum_{i = 1}^n a_i \hat y_i
    \]
    . Рассмотрим функцию
    \[
        z(x) = \sum_{i = 1}^n a_i y_i
    \]
    Заметим, что $z(x)$ является решением системы $y' = A(x)y$. Но $z(x_0) = y(x_0)$. А поскольку решение задачи Коши
    единственно, то $y \equiv z$. Утверждение доказано.
\end{proof}

\end{document}
